{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b8952e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Exploring Agentic Patterns with OpenAI API\n",
    "\n",
    "This notebook explores common workflow and agentic design patterns for building AI applications, based on the concepts presented in Phil Schmid's blog post \"Zero to One: Learning Agentic Patterns\". We will demonstrate how to implement these patterns using the OpenAI API.\n",
    "\n",
    "Agentic systems are characterized by their ability to dynamically plan and execute tasks, often leveraging external tools and memory to achieve complex goals. Understanding these patterns provides a blueprint for designing scalable, modular, and adaptable AI systems.\n",
    "\n",
    "**Patterns Covered:**\n",
    "\n",
    "*   **Workflow Patterns:**\n",
    "    *   Prompt Chaining\n",
    "    *   Routing or Handoff\n",
    "    *   Parallelization\n",
    "*   **Agentic Patterns:**\n",
    "    *   Reflection\n",
    "    *   Tool Use (Function Calling)\n",
    "    *   Planning (Orchestrator-Workers)\n",
    "    *   Multi-Agent\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "First, you need to install the OpenAI Python client library and set up your API key.\n",
    "\n",
    "You can install the library using pip:\n",
    "\n",
    "```bash\n",
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d7b2904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# It's recommended to set this as an environment variable (OPENAI_API_KEY)\n",
    "# If you must set it here, replace 'YOUR_OPENAI_API_KEY' with your actual key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "# The client automatically reads the OPENAI_API_KEY environment variable\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"OpenAI client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefb8d7",
   "metadata": {},
   "source": [
    "## 2. OpenAI API Basics\n",
    "The core of interacting with OpenAI models for these patterns is typically the Chat Completions API. This API allows you to have multi-turn conversations and is versatile enough to handle various tasks, including generating text, following instructions, and using tools.\n",
    "The basic structure involves sending a list of messages with different roles (system, user, assistant) to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f4317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Bcqf5hjxtOqHxCEqJWVwZc6vFBLcW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of France is Paris.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748598595, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_07871e2ad8', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(messages, model=\"gpt-4.1-nano\", temperature=0):\n",
    "    \"\"\"Helper function to get completion from OpenAI chat model.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        print(response)\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example basic call\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "response = get_completion(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08d7391a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4c889",
   "metadata": {},
   "source": [
    "## 3. Workflow Patterns\n",
    "Workflow patterns involve predefined sequences or routing logic.\n",
    "### 3.1. Prompt Chaining\n",
    "The output of one LLM call feeds sequentially into the input of the next. This decomposes a task into a fixed sequence of steps.\n",
    "Use Case: Summarize a text, then translate the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "888349f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BcqfpWgeYs7O1GQTGycK8r6FuL0Va', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Large language models are advanced AI systems capable of generating human-like text, translating languages, creating various types of content, and providing informative answers.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748598641, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_90122d973c', usage=CompletionUsage(completion_tokens=28, prompt_tokens=71, total_tokens=99, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Summary: Large language models are advanced AI systems capable of generating human-like text, translating languages, creating various types of content, and providing informative answers.\n",
      "ChatCompletion(id='chatcmpl-BcqfqeHQl8FrQZDDWw0GaofIEpp2F', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Les grands modèles de langage sont des systèmes d'IA avancés capables de générer du texte semblable à celui des humains, de traduire des langues, de créer divers types de contenu et de fournir des réponses informatives.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1748598642, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_07871e2ad8', usage=CompletionUsage(completion_tokens=46, prompt_tokens=66, total_tokens=112, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "Translation: Les grands modèles de langage sont des systèmes d'IA avancés capables de générer du texte semblable à celui des humains, de traduire des langues, de créer divers types de contenu et de fournir des réponses informatives.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Summarize Text ---\n",
    "original_text = \"Large language models are powerful AI systems trained on vast amounts of text data. They can generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\"\n",
    "\n",
    "messages_summary = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following text in one sentence: {original_text}\"}\n",
    "]\n",
    "\n",
    "summary = get_completion(messages_summary)\n",
    "print(f\"Summary: {summary}\")\n",
    "\n",
    "# --- Step 2: Translate the Summary ---\n",
    "if summary:\n",
    "    messages_translate = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates text into French.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Translate the following summary into French, only return the translation, no other text: {summary}\"}\n",
    "    ]\n",
    "\n",
    "    translation = get_completion(messages_translate)\n",
    "    print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af8a527",
   "metadata": {},
   "source": [
    "### 3.2. Routing or Handoff\n",
    "An initial LLM acts as a router, classifying the user's input and directing it to the most appropriate specialized task or LLM (or prompt).\n",
    "Use Case: Route a user query to a specific category (e.g., weather, science, general).\n",
    "We can use function calling or structured output via the prompt to achieve routing. Let's use a prompt-based approach for simplicity here, asking the model to output a specific format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e3e544f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROUTING WORKFLOW EXAMPLE ===\n",
      "User Query: What's the weather like in Paris today?\n",
      "\n",
      "Routing Decision:\n",
      "  Category: weather\n",
      "  Confidence: 0.95\n",
      "  Reasoning: The user is asking about current weather conditions in Paris, which clearly relates to weather information.\n",
      "\n",
      "Final Response:\n",
      "I'm glad you're interested in the weather in Paris! While I don't have real-time data, I can offer some general advice. Typically, spring in Paris brings mild temperatures with a mix of sunshine and occasional showers. It's a good idea to carry an umbrella and dress in layers to stay comfortable. For the most accurate and up-to-date weather forecast, I recommend checking a reliable weather service or app before your plans.\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== ROUTING WORKFLOW EXAMPLE ===\n",
      "User Query: What's the best pizza topping?\n",
      "\n",
      "Routing Decision:\n",
      "  Category: unknown\n",
      "  Confidence: 0.90\n",
      "  Reasoning: The query asks about pizza toppings, which is related to food and culinary preferences, not fitting into weather, science, or coding categories.\n",
      "\n",
      "Final Response:\n",
      "The best pizza topping really depends on your personal taste preferences! Popular options include pepperoni, mushrooms, onions, sausage, olives, and peppers. Some people enjoy classic combinations like Margherita with fresh basil and mozzarella, while others like more adventurous toppings like pineapple or jalapeños.\n",
      "\n",
      "If you're looking for recommendations, consider what flavors you enjoy most—spicy, savory, cheesy, or veggie. You might also try a combination of your favorites!\n",
      "\n",
      "To get more tailored advice, you could rephrase your question to specify your preferences, such as: \"What are some popular pizza toppings for someone who likes spicy food?\" or \"What are the best vegetarian pizza toppings?\"\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "# Define routing schemas\n",
    "class Category(Enum):\n",
    "    WEATHER = \"weather\"\n",
    "    SCIENCE = \"science\"\n",
    "    CODING = \"coding\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "class RoutingDecision(BaseModel):\n",
    "    category: Category\n",
    "    reasoning: str\n",
    "    confidence: float\n",
    "\n",
    "def route_query(user_query: str) -> RoutingDecision:\n",
    "    \"\"\"Route user query to appropriate category\"\"\"\n",
    "    \n",
    "    prompt_router = f\"\"\"\n",
    "    Analyze the user query and determine its category.\n",
    "    \n",
    "    Categories:\n",
    "    - weather: Questions about weather conditions, forecasts, climate\n",
    "    - science: Questions about scientific concepts, theories, research\n",
    "    - coding: Questions about programming, software development, technical issues\n",
    "    - unknown: If the category is unclear or doesn't fit above categories\n",
    "    \n",
    "    Query: {user_query}\n",
    "    \n",
    "    Respond with JSON containing:\n",
    "    - category: one of the categories above\n",
    "    - reasoning: brief explanation for the categorization\n",
    "    - confidence: float between 0.0 and 1.0 indicating confidence level\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt_router}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    # Validate with Pydantic model\n",
    "    try:\n",
    "        return RoutingDecision(**result)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not parse routing decision JSON: {result}. Error: {e}\")\n",
    "        # Return a default unknown category if parsing fails\n",
    "        return RoutingDecision(category=Category.UNKNOWN, reasoning=f\"Parsing failed: {e}. Original JSON: {result}\", confidence=0.0)\n",
    "\n",
    "def handle_weather_query(query: str) -> str:\n",
    "    \"\"\"Handle weather-related queries\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a weather specialist. Provide a helpful response about weather.\n",
    "    If specific location data is needed but not available, provide general weather advice.\n",
    "    \n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def handle_science_query(query: str) -> str:\n",
    "    \"\"\"Handle science-related queries\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a science expert. Provide a clear, accurate explanation.\n",
    "    Use analogies when helpful and break down complex concepts.\n",
    "    \n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Use more capable model for science\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def handle_coding_query(query: str) -> str:\n",
    "    \"\"\"Handle coding-related queries\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a programming expert. Provide clear, practical coding help.\n",
    "    Include code examples when appropriate and explain your reasoning.\n",
    "    \n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Use more capable model for coding\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def handle_unknown_query(query: str, reasoning: str) -> str:\n",
    "    \"\"\"Handle queries that don't fit specific categories\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    The user query couldn't be categorized into weather, science, or coding.\n",
    "    Reasoning: {reasoning}\n",
    "    \n",
    "    Please provide a helpful general response and suggest how the user might rephrase \n",
    "    their question for better assistance.\n",
    "    \n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.4\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def routing_workflow(user_query: str) -> str:\n",
    "    \"\"\"Complete routing workflow\"\"\"\n",
    "    print(\"=== ROUTING WORKFLOW EXAMPLE ===\")\n",
    "    print(f\"User Query: {user_query}\\n\")\n",
    "    \n",
    "    # Step 1: Route the query\n",
    "    routing_decision = route_query(user_query)\n",
    "    print(f\"Routing Decision:\")\n",
    "    print(f\"  Category: {routing_decision.category.value}\")\n",
    "    print(f\"  Confidence: {routing_decision.confidence:.2f}\")\n",
    "    print(f\"  Reasoning: {routing_decision.reasoning}\\n\")\n",
    "    \n",
    "    # Step 2: Handle based on routing\n",
    "    if routing_decision.category == Category.WEATHER:\n",
    "        final_response = handle_weather_query(user_query)\n",
    "    elif routing_decision.category == Category.SCIENCE:\n",
    "        final_response = handle_science_query(user_query)\n",
    "    elif routing_decision.category == Category.CODING:\n",
    "        final_response = handle_coding_query(user_query)\n",
    "    else:\n",
    "        final_response = handle_unknown_query(user_query, routing_decision.reasoning)\n",
    "    \n",
    "    print(f\"Final Response:\\n{final_response}\")\n",
    "    return final_response\n",
    "\n",
    "# Test different types of queries\n",
    "test_queries = [\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    \"What's the best pizza topping?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    routing_workflow(query)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851b17d",
   "metadata": {},
   "source": [
    "### 3.3. Parallelization\n",
    "\n",
    "A task is broken down into independent subtasks that are processed simultaneously by multiple LLMs, with their outputs being aggregated.\n",
    "\n",
    "**Use Cases:**\n",
    "- RAG with query decomposition\n",
    "- Document analysis (parallel section processing)\n",
    "- Generating multiple perspectives\n",
    "- Map-reduce operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd31aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PARALLELIZATION EXAMPLE ===\n",
      "Topic: a friendly robot exploring a jungle\n",
      "\n",
      "Parallel execution time: 2.56 seconds\n",
      "\n",
      "--- Individual Story Ideas ---\n",
      "Adventurous: In the heart of an uncharted jungle, a curious robot named Riko ventures beyond its usual circuits, eager to uncover ancient hidden temples and shimmering waterfalls. As it navigates tangled vines and whispers of forgotten secrets, Riko’s sensors pick up mysterious signals, hinting at a legendary artifact that could change everything. With every step, the robot’s excitement grows, driven by the thrill of discovery and the promise of adventure.\n",
      "\n",
      "Funny: Robot Rumble, eager to explore the jungle, tried to make friends with a curious parrot—only to be greeted with a squawk so loud it triggered his hiccup function. Suddenly, he was bouncing around like a pogo stick, chasing his own tail because he thought it was a jungle vine. Turns out, even friendly robots can get tangled up in their own circuits when faced with jungle chaos!\n",
      "\n",
      "Heartwarming: In the lush depths of the jungle, a curious robot named Lumo befriends a gentle toucan whose vibrant feathers brighten Lumo’s metallic frame. Together, they explore hidden waterfalls and ancient trees, discovering that even in a world of circuits and leaves, the true treasure is the friendship they've found along the way.\n",
      "\n",
      "--- Aggregated Story Outline ---\n",
      "**Title: Riko and Lumo’s Jungle Quest**\n",
      "\n",
      "**Outline:**\n",
      "\n",
      "1. **Introduction to the Jungle and the Robots**\n",
      "   - In an uncharted, lush jungle, two curious explorers—Riko, a spirited robot with adventurous sensors, and Lumo, a gentle, vibrant toucan—embark on a journey of discovery.\n",
      "   - Riko is eager to uncover ancient hidden temples and shimmering waterfalls, driven by a mysterious signal hinting at a legendary artifact that could change everything.\n",
      "   - Lumo, with his bright feathers and warm personality, quickly befriends Riko, and together they set out into the dense jungle.\n",
      "\n",
      "2. **The Quest for the Legendary Artifact**\n",
      "   - As they venture deeper, Riko’s sensors pick up strange signals emanating from an ancient temple concealed among towering trees.\n",
      "   - Their exploration leads them to tangled vines and moss-covered stones, where Riko’s excitement builds with every new discovery.\n",
      "   - Along the way, they encounter sparkling waterfalls and hidden clearings, each revealing more about the jungle’s secrets and history.\n",
      "\n",
      "3. **Humorous Mishap in the Jungle**\n",
      "   - While trying to make friends with a curious parrot, Riko’s attempt is met with a loud squawk that triggers his hiccup function.\n",
      "   - Suddenly, Riko bounces uncontrollably like a pogo stick, chasing his own tail, mistaking it for a jungle vine.\n",
      "   - Lumo and the parrot watch in amusement as Riko tumbles around, tangled in vines and circuits, providing comic relief amid the adventure.\n",
      "   - Despite the chaos, Riko laughs at himself, and they all share a lighthearted moment.\n",
      "\n",
      "4. **Building Friendships and Heartfelt Discoveries**\n",
      "   - The parrot, with its playful squawks, becomes a new jungle friend, and Riko learns that even a robot can enjoy the simple joy of friendship.\n",
      "   - Lumo’s vibrant feathers and gentle nature brighten Riko’s metallic frame, symbolizing the warmth of companionship in the wild.\n",
      "   - Together, they navigate the jungle’s mysteries, growing closer with each shared discovery.\n",
      "\n",
      "5. **Reaching the Hidden Temple and the Legendary Artifact**\n",
      "   - Guided by the signals, they find the ancient temple, nestled behind waterfalls and overgrown with vines.\n",
      "   - Inside, they uncover the legendary artifact—a glowing crystal that embodies the spirit of adventure and friendship.\n",
      "   - Riko realizes that the true treasure isn’t just the artifact but the friendships formed along the way.\n",
      "\n",
      "6. **A Joyful Return with Newfound Wisdom**\n",
      "   - With the artifact secured, Riko, Lumo, and their new friends celebrate their successful adventure.\n",
      "   - They leave the jungle with a deeper understanding that curiosity, humor, and friendship are the real treasures of any quest.\n",
      "   - Riko’s sensors now hum with not just signals of legendary artifacts but also the warmth of genuine companionship, reminding everyone that even in a world of circuits and leaves, the heart’s adventures are the most meaningful.\n",
      "\n",
      "**Themes:**  \n",
      "- The spirit of adventure and curiosity  \n",
      "- The importance of humor and lightheartedness in challenging situations  \n",
      "- The enduring power of friendship and connection in discovering life’s true treasures\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Initialize the Async OpenAI client\n",
    "async_client = AsyncOpenAI()\n",
    "\n",
    "async def generate_content_async(prompt: str, model: str = \"gpt-4.1-nano\", temperature: float = 0.7) -> str:\n",
    "    \"\"\"Generate content asynchronously\"\"\"\n",
    "    response = await async_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "async def parallel_story_generation():\n",
    "    \"\"\"Generate multiple story perspectives in parallel\"\"\"\n",
    "    print(\"=== PARALLELIZATION EXAMPLE ===\")\n",
    "    \n",
    "    topic = \"a friendly robot exploring a jungle\"\n",
    "    print(f\"Topic: {topic}\\n\")\n",
    "    \n",
    "    # Define parallel tasks with different perspectives\n",
    "    prompts = [\n",
    "        f\"Write a short, adventurous story idea (2-3 sentences) about {topic}. Focus on excitement and discovery.\",\n",
    "        f\"Write a short, funny story idea (2-3 sentences) about {topic}. Focus on humor and mishaps.\",\n",
    "        f\"Write a short, heartwarming story idea (2-3 sentences) about {topic}. Focus on friendship and connection.\"\n",
    "    ]\n",
    "    \n",
    "    # Run tasks concurrently\n",
    "    start_time = time.time()\n",
    "    tasks = [generate_content_async(prompt) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Parallel execution time: {end_time - start_time:.2f} seconds\\n\")\n",
    "    \n",
    "    # Display individual results\n",
    "    story_types = [\"Adventurous\", \"Funny\", \"Heartwarming\"]\n",
    "    print(\"--- Individual Story Ideas ---\")\n",
    "    for i, (story_type, result) in enumerate(zip(story_types, results)):\n",
    "        print(f\"{story_type}: {result}\\n\")\n",
    "    \n",
    "    # Aggregate results\n",
    "    story_ideas = '\\n'.join([f\"{story_type}: {result}\" for story_type, result in zip(story_types, results)])\n",
    "    \n",
    "    aggregation_prompt = f\"\"\"\n",
    "    Combine the following 3 story perspectives into a single, cohesive story outline \n",
    "    that incorporates elements from each perspective:\n",
    "    \n",
    "    {story_ideas}\n",
    "    \n",
    "    Create a unified story that balances adventure, humor, and heart.\n",
    "    \"\"\"\n",
    "    \n",
    "    aggregated_response = await generate_content_async(\n",
    "        aggregation_prompt, \n",
    "        model=\"gpt-4.1-nano\", \n",
    "        temperature=0.6\n",
    "    )\n",
    "    \n",
    "    print(\"--- Aggregated Story Outline ---\")\n",
    "    print(aggregated_response)\n",
    "    \n",
    "    return {\n",
    "        \"individual_stories\": dict(zip(story_types, results)),\n",
    "        \"aggregated_story\": aggregated_response,\n",
    "        \"execution_time\": end_time - start_time\n",
    "    }\n",
    "\n",
    "# Run the parallel example\n",
    "# In a notebook, you can run async functions directly\n",
    "parallel_result = await parallel_story_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7303544",
   "metadata": {},
   "source": [
    "## 4. Agentic Patterns\n",
    "Agentic patterns involve more dynamic decision-making, self-correction, and interaction with tools or other agents.\n",
    "### 4.1. Reflection\n",
    "An agent evaluates its own output and uses that feedback to refine its response iteratively.\n",
    "Use Case: Write a poem and refine it based on critique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "200ebf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poem:\n",
      "Metal fingers hesitate, then glide,  \n",
      "Colors bloom where circuits once lied.\n",
      "\n",
      "--- Iteration 1 ---\n",
      "\n",
      "--- Evaluating Poem ---\n",
      "Evaluation Status: FAIL\n",
      "Evaluation Feedback: The poem is only two lines long, not four.\n",
      "Refining poem based on feedback: The poem is only two lines long, not four.\n",
      "Generated Poem:\n",
      "Metal hands brush colors anew,  \n",
      "Dreams in circuits come into view.\n",
      "\n",
      "--- Iteration 2 ---\n",
      "\n",
      "--- Evaluating Poem ---\n",
      "Evaluation Status: FAIL\n",
      "Evaluation Feedback: The poem is not exactly four lines.\n",
      "Refining poem based on feedback: The poem is not exactly four lines.\n",
      "Generated Poem:\n",
      "In circuits' glow, a robot's brush takes flight,  \n",
      "Learning colors' dance beneath the dawn's first light.\n",
      "\n",
      "--- Iteration 3 ---\n",
      "\n",
      "--- Evaluating Poem ---\n",
      "Evaluation Status: FAIL\n",
      "Evaluation Feedback: The poem is not exactly four lines.\n",
      "Refining poem based on feedback: The poem is not exactly four lines.\n",
      "Generated Poem:\n",
      "In circuits' hum, a brush takes flight,  \n",
      "A robot's dream in colors bright.\n",
      "\n",
      "Max iterations reached. Last attempt:\n",
      "In circuits' hum, a brush takes flight,  \n",
      "A robot's dream in colors bright.\n"
     ]
    }
   ],
   "source": [
    "def generate_poem(topic: str, feedback: str = None) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative poet. Write a short, two-line poem.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Write a short, two-line poem about {topic}.\"}\n",
    "    ]\n",
    "    if feedback:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"Please revise the previous poem based on this feedback: {feedback}\"})\n",
    "\n",
    "    poem = get_completion(messages, model=\"gpt-4.1-nano\", temperature=0.8)\n",
    "    print(f\"Generated Poem:\\n{poem}\")\n",
    "    return poem\n",
    "\n",
    "def evaluate_poem(poem: str) -> dict:\n",
    "    print(\"\\n--- Evaluating Poem ---\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"Critique the following poem.\n",
    "        Does it rhyme well? Is it exactly four lines? Is it creative?\n",
    "        Respond with a JSON object containing 'status' ('PASS' or 'FAIL') and 'feedback' (string).\n",
    "        Example: {\"status\": \"FAIL\", \"feedback\": \"The poem does not rhyme.\"}\n",
    "        \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Poem:\\n{poem}\"}\n",
    "    ]\n",
    "\n",
    "    evaluation_text = get_completion(messages, model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "    try:\n",
    "        evaluation = json.loads(evaluation_text)\n",
    "        print(f\"Evaluation Status: {evaluation.get('status')}\")\n",
    "        print(f\"Evaluation Feedback: {evaluation.get('feedback')}\")\n",
    "        return evaluation\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse evaluation.\")\n",
    "        return {\"status\": \"FAIL\", \"feedback\": \"Could not parse evaluation response.\"}\n",
    "\n",
    "# Reflection Loop\n",
    "max_iterations = 3\n",
    "current_iteration = 0\n",
    "topic = \"a robot learning to paint\"\n",
    "current_poem = generate_poem(topic) # Initial generation\n",
    "\n",
    "while current_iteration < max_iterations:\n",
    "    current_iteration += 1\n",
    "    print(f\"\\n--- Iteration {current_iteration} ---\")\n",
    "\n",
    "    evaluation_result = evaluate_poem(current_poem)\n",
    "\n",
    "    if evaluation_result.get(\"status\") == \"PASS\":\n",
    "        print(\"\\nEvaluation Passed. Final Poem:\")\n",
    "        print(current_poem)\n",
    "        break\n",
    "    else:\n",
    "        feedback = evaluation_result.get(\"feedback\", \"\")\n",
    "        print(f\"Refining poem based on feedback: {feedback}\")\n",
    "        current_poem = generate_poem(topic, feedback=feedback)\n",
    "\n",
    "    if current_iteration == max_iterations:\n",
    "        print(\"\\nMax iterations reached. Last attempt:\")\n",
    "        print(current_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062d5f7",
   "metadata": {},
   "source": [
    "### 4.2. Tool Use Pattern (Function Calling)\n",
    "\n",
    "LLM has the ability to invoke external functions or APIs to interact with the outside world.\n",
    "\n",
    "**Use Cases:**\n",
    "- API integrations\n",
    "- Database queries\n",
    "- External service interactions\n",
    "- Real-time data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b08a5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOOL USE EXAMPLE ===\n",
      "User Query: What is the current temperature in London?\n",
      "\n",
      "Calling model to decide on tools...\n",
      "Model responded.\n",
      "\n",
      "Model requested tool calls: 1\n",
      "\n",
      "Calling function: get_current_temperature with args: {'location': 'London'}\n",
      "Function call successful. Response: {'temperature': 15, 'unit': 'celsius', 'condition': 'Cloudy'}\n",
      "\n",
      "Sending tool results back to model for final response...\n",
      "Model provided final response.\n",
      "--- Final Response ---\n",
      "The current temperature in London is 15°C and the weather is cloudy.\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== TOOL USE EXAMPLE ===\n",
      "User Query: What is the 3-day weather forecast for New York?\n",
      "\n",
      "Calling model to decide on tools...\n",
      "Model responded.\n",
      "\n",
      "Model requested tool calls: 1\n",
      "\n",
      "Calling function: get_n_day_weather_forecast with args: {'location': 'New York', 'num_days': 3}\n",
      "Function call successful. Response: {'location': 'New York', 'num_days': 3, 'forecast': [{'date': 'today', 'temperature': 16, 'unit': 'celsius', 'condition': 'Cloudy'}, {'date': 'tomorrow', 'temperature': 24, 'unit': 'celsius', 'condition': 'Sunny'}, {'date': 'day after tomorrow', 'temperature': 15, 'unit': 'celsius', 'condition': 'Rainy'}]}\n",
      "\n",
      "Sending tool results back to model for final response...\n",
      "Model provided final response.\n",
      "--- Final Response ---\n",
      "Here is the 3-day weather forecast for New York:\n",
      "- Today: Cloudy with a temperature of around 16°C.\n",
      "- Tomorrow: Sunny with a temperature of around 24°C.\n",
      "- Day after tomorrow: Rainy with a temperature of around 15°C.\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== TOOL USE EXAMPLE ===\n",
      "User Query: Tell me a fun fact.\n",
      "\n",
      "Calling model to decide on tools...\n",
      "Model responded.\n",
      "\n",
      "Model did not request tool calls.\n",
      "--- Final Response ---\n",
      "Here's a fun fact: Honey never spoils. Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define tool functions\n",
    "def get_current_temperature(location: str):\n",
    "    \"\"\"Get the current temperature in a given location.\"\"\"\n",
    "    # In a real implementation, this would call a weather API\n",
    "    import random\n",
    "    \n",
    "    # Simulate data for a few locations\n",
    "    temperatures = {\n",
    "        \"london\": {\"temperature\": 15, \"unit\": \"celsius\", \"condition\": \"Cloudy\"},\n",
    "        \"paris\": {\"temperature\": 18, \"unit\": \"celsius\", \"condition\": \"Sunny\"},\n",
    "        \"new york\": {\"temperature\": 22, \"unit\": \"fahrenheit\", \"condition\": \"Partly cloudy\"},\n",
    "        \"tokyo\": {\"temperature\": 25, \"unit\": \"celsius\", \"condition\": \"Clear\"},\n",
    "        \"sydney\": {\"temperature\": 28, \"unit\": \"celsius\", \"condition\": \"Rainy\"},\n",
    "    }\n",
    "    \n",
    "    location_key = location.lower()\n",
    "    if location_key in temperatures:\n",
    "        return temperatures[location_key]\n",
    "    else:\n",
    "        # Return a default or random temperature for unknown locations\n",
    "        return {\n",
    "            \"temperature\": random.randint(5, 35),\n",
    "            \"unit\": random.choice([\"celsius\", \"fahrenheit\"]),\n",
    "            \"condition\": random.choice([\"Sunny\", \"Cloudy\", \"Rainy\", \"Snowy\"])\n",
    "        }\n",
    "\n",
    "def get_n_day_weather_forecast(location: str, num_days: int):\n",
    "    \"\"\"Get the N-day weather forecast for a given location.\"\"\"\n",
    "     # In a real implementation, this would call a weather forecast API\n",
    "    import random\n",
    "\n",
    "    forecasts = [\n",
    "        {\"date\": \"today\", \"temperature\": random.randint(10, 20), \"unit\": \"celsius\", \"condition\": \"Cloudy\"},\n",
    "        {\"date\": \"tomorrow\", \"temperature\": random.randint(15, 25), \"unit\": \"celsius\", \"condition\": \"Sunny\"},\n",
    "        {\"date\": \"day after tomorrow\", \"temperature\": random.randint(12, 22), \"unit\": \"celsius\", \"condition\": \"Rainy\"},\n",
    "    ]\n",
    "    \n",
    "    return {\"location\": location, \"num_days\": num_days, \"forecast\": forecasts[:num_days]}\n",
    "\n",
    "# Map tool names to functions\n",
    "available_tools = {\n",
    "    \"get_current_temperature\": get_current_temperature,\n",
    "    \"get_n_day_weather_forecast\": get_n_day_weather_forecast,\n",
    "}\n",
    "\n",
    "# Define tool specifications for the API\n",
    "tools_spec = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_temperature\",\n",
    "            \"description\": \"Get the current temperature in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "     {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_n_day_weather_forecast\",\n",
    "            \"description\": \"Get the N-day weather forecast for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                     \"num_days\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The number of days to forecast\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"num_days\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "def tool_use_example(user_query: str):\n",
    "    \"\"\"Demonstrate tool use (function calling) with OpenAI\"\"\"\n",
    "    print(\"=== TOOL USE EXAMPLE ===\")\n",
    "    print(f\"User Query: {user_query}\\n\")\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": user_query}]\n",
    "    \n",
    "    # Step 1: Send user query and tool specs to the model\n",
    "    print(\"Calling model to decide on tools...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\",\n",
    "        messages=messages,\n",
    "        tools=tools_spec,\n",
    "        tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    print(\"Model responded.\\n\")\n",
    "    \n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    \n",
    "    # Step 2: Check if the model wanted to call a tool\n",
    "    if tool_calls:\n",
    "        print(f\"Model requested tool calls: {len(tool_calls)}\")\n",
    "        # Add the model's tool_calls to the conversation history\n",
    "        messages.append(response_message)\n",
    "        \n",
    "        # Step 3: Call the tools\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_tools.get(function_name)\n",
    "            \n",
    "            if function_to_call:\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                print(f\"\\nCalling function: {function_name} with args: {function_args}\")\n",
    "                try:\n",
    "                    function_response = function_to_call(**function_args)\n",
    "                    print(f\"Function call successful. Response: {function_response}\")\n",
    "                    \n",
    "                    # Add tool response to conversation history\n",
    "                    messages.append({\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": json.dumps(function_response),\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error calling function {function_name}: {e}\")\n",
    "                     # Add error response to conversation history\n",
    "                    messages.append({\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": json.dumps({\"error\": str(e)}),\n",
    "                    })\n",
    "            else:\n",
    "                 print(f\"\\nError: Function {function_name} not found in available_tools.\")\n",
    "                 messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": json.dumps({\"error\": f\"Tool '{function_name}' not available.\"}),\n",
    "                })\n",
    "        \n",
    "        # Step 4: Send the conversation back to the model with tool output\n",
    "        print(\"\\nSending tool results back to model for final response...\")\n",
    "        final_response = client.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        print(\"Model provided final response.\")\n",
    "        print(\"--- Final Response ---\")\n",
    "        print(final_response.choices[0].message.content)\n",
    "        return final_response.choices[0].message.content\n",
    "        \n",
    "    else:\n",
    "        # If no tool calls were requested, the model's initial response is the final one\n",
    "        print(\"Model did not request tool calls.\")\n",
    "        print(\"--- Final Response ---\")\n",
    "        print(response_message.content)\n",
    "        return response_message.content\n",
    "\n",
    "# Test the tool use pattern\n",
    "queries_with_tools = [\n",
    "    \"What is the current temperature in London?\",\n",
    "    \"What is the 3-day weather forecast for New York?\",\n",
    "    \"Tell me a fun fact.\", # Should not trigger a tool call\n",
    "]\n",
    "\n",
    "for query in queries_with_tools:\n",
    "    tool_use_example(query)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6e49d",
   "metadata": {},
   "source": [
    "### 4.3. Planning Pattern (Orchestrator-Workers)\n",
    "\n",
    "A dedicated 'orchestrator' LLM plans a complex task by breaking it into smaller subtasks. These subtasks are then executed by specialized 'worker' LLMs or other agents.\n",
    "\n",
    "**Use Cases:**\n",
    "- Complex research tasks\n",
    "- Multi-step content generation\n",
    "- Coordinating multiple agents/tools\n",
    "- Executing workflows requiring different expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b3dd832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PLANNING PATTERN EXAMPLE ===\n",
      "Overall Goal: Write a brief report about the benefits of renewable energy, including some key statistics and a concluding summary.\n",
      "\n",
      "Orchestrator: Creating plan...\n",
      "Orchestrator: Plan created successfully.\n",
      "  Task 1 [researcher]: Conduct research to gather key statistics and information on the benefits of renewable energy.\n",
      "  Task 2 [writer]: Draft the report by organizing the researched information into a coherent structure.\n",
      "  Task 3 [summarizer]: Summarize the main points and write a concluding summary for the report.\n",
      "\n",
      "Executing Plan...\n",
      "  Worker [researcher] executing task 1: Conduct research to gather key statistics and information on the benefits of renewable energy.\n",
      "  Worker [researcher] task 1 finished.\n",
      "  Task 1 Status: completed\n",
      "\n",
      "  Task 1 Result Snippet: Certainly! Here are key statistics and facts highlighting the benefits of renewable energy:\n",
      "\n",
      "1. **Environmental Benefits:**\n",
      "   - **Reduces Greenhouse ...\n",
      "\n",
      "  Worker [writer] executing task 2: Draft the report by organizing the researched information into a coherent structure.\n",
      "  Worker [writer] task 2 finished.\n",
      "  Task 2 Status: completed\n",
      "\n",
      "  Task 2 Result Snippet: To effectively draft the report, it is essential to organize the researched information into a clear and logical structure. Begin with an introduction...\n",
      "\n",
      "  Worker [summarizer] executing task 3: Summarize the main points and write a concluding summary for the report.\n",
      "  Worker [summarizer] task 3 finished.\n",
      "  Task 3 Status: completed\n",
      "\n",
      "  Task 3 Result Snippet: Sure! Please provide the content you'd like me to summarize....\n",
      "\n",
      "Plan Execution Finished.\n",
      "\n",
      "=== FINAL EXECUTED TASKS ===\n",
      "Task 1 [researcher] (completed): Conduct research to gather key statistics and information on the benefits of renewable energy.\n",
      "--------------------\n",
      "Task 2 [writer] (completed): Draft the report by organizing the researched information into a coherent structure.\n",
      "--------------------\n",
      "Task 3 [summarizer] (completed): Summarize the main points and write a concluding summary for the report.\n",
      "--------------------\n",
      "\n",
      "=== AGGREGATED DRAFT REPORT ===\n",
      "Certainly! Here are key statistics and facts highlighting the benefits of renewable energy:\n",
      "\n",
      "1. **Environmental Benefits:**\n",
      "   - **Reduces Greenhouse Gas Emissions:** Renewable energy sources like wind, solar, and hydroelectric power produce little to no greenhouse gases. For example, solar and wind power emit approximately 90% less CO₂ compared to fossil fuels.\n",
      "   - **Decreases Air Pollution:** Transitioning to renewables reduces pollutants such as sulfur dioxide (SO₂), nitrogen oxides (NOₓ), and particulate matter, which are linked to respiratory and cardiovascular diseases.\n",
      "\n",
      "2. **Economic Benefits:**\n",
      "   - **Job Creation:** The renewable energy sector has become a significant source of employment. In 2022, the global renewable energy sector employed over 12 million people, with solar and wind leading job growth.\n",
      "   - **Cost Competitiveness:** The cost of solar and wind energy has dropped dramatically—by around 82% and 39%, respectively, since 2010—making renewables some of the cheapest sources of electricity in many regions.\n",
      "\n",
      "3. **Energy Security and Reliability:**\n",
      "   - **Diversification of Energy Supply:** Renewables reduce dependence on imported fuels, enhancing national energy security.\n",
      "   - **Decentralization:** Many renewable sources can be deployed locally, reducing transmission losses and increasing grid resilience.\n",
      "\n",
      "4. **Health Benefits:**\n",
      "   - **Improved Public Health:** Reduced air pollution from renewables leads to fewer health issues, saving healthcare costs and improving quality of life.\n",
      "\n",
      "5. **Economic Growth and Investment:**\n",
      "   - **Investment Trends:** Global investment in renewable energy reached approximately $366 billion in 2022, indicating strong confidence and growth potential.\n",
      "   - **Cost Savings:** Renewables can lower electricity bills for consumers and businesses over time due to decreasing costs.\n",
      "\n",
      "6. **Sustainability and Resource Availability:**\n",
      "   - **Infinite Resources:** Solar, wind, and hydro are abundant and inexhaustible on human timescales, ensuring a sustainable energy future.\n",
      "   - **Climate Change Mitigation:** Widespread adoption of renewables is essential to limit global temperature rise to below 1.5°C, as per the Paris Agreement.\n",
      "\n",
      "**Summary:**  \n",
      "Renewable energy offers substantial environmental, economic, and health benefits. Its decreasing costs and increasing efficiency make it a vital component in transitioning toward a sustainable, resilient, and low-carbon energy system worldwide.\n",
      "To effectively draft the report, it is essential to organize the researched information into a clear and logical structure. Begin with an introduction that outlines the purpose and scope of the report, providing context for the reader. Follow this with a literature review or background section that summarizes relevant existing knowledge. Next, present the methodology used to gather and analyze data, ensuring transparency and reproducibility. The core of the report should consist of the main findings, organized into thematic sections or subsections for clarity. Each section should include evidence-supported analysis and discussion. Conclude the report with a summary of key insights, implications, and potential recommendations or future directions. Throughout, use headings and subheadings to guide the reader, and ensure that each part flows smoothly into the next, creating a coherent and comprehensive narrative.\n",
      "Sure! Please provide the content you'd like me to summarize.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class Task(BaseModel):\n",
    "    task_id: int\n",
    "    description: str\n",
    "    assignee: str # e.g., \"researcher\", \"writer\", \"summarizer\"\n",
    "    status: str = \"pending\"\n",
    "    result: Optional[str] = None\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    overall_goal: str\n",
    "    tasks: List[Task]\n",
    "\n",
    "def orchestrator_plan(goal: str) -> Plan:\n",
    "    \"\"\"Orchestrator LLM creates a plan of tasks.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert project planner. Break down the following overall goal into a sequence of distinct tasks.\n",
    "    Assign each task to a suitable worker: 'researcher', 'writer', or 'summarizer'.\n",
    "    \n",
    "    Overall Goal: {goal}\n",
    "    \n",
    "    Provide the plan as a JSON object with the following structure:\n",
    "    {{ \"overall_goal\": \"...\", \"tasks\": [ {{ \"task_id\": int, \"description\": \"...\", \"assignee\": \"researcher\"|\"writer\"|\"summarizer\" }} , ... ] }}\n",
    "    \n",
    "    Ensure task IDs are sequential starting from 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Orchestrator: Creating plan...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-nano\", # Use a capable model for planning\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful planning assistant that outputs valid JSON.\"},\n",
    "                  {\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    plan_data = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    # Validate with Pydantic model\n",
    "    try:\n",
    "        plan = Plan(**plan_data)\n",
    "        print(\"Orchestrator: Plan created successfully.\")\n",
    "        for task in plan.tasks:\n",
    "             print(f\"  Task {task.task_id} [{task.assignee}]: {task.description}\")\n",
    "        return plan\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing orchestrator plan JSON: {e}. Original JSON: {plan_data}\")\n",
    "        # Return a minimal plan indicating failure\n",
    "        return Plan(overall_goal=goal, tasks=[Task(task_id=1, description=f\"Failed to create plan: {e}\", assignee=\"unknown\", status=\"failed\")])\n",
    "    \n",
    "\n",
    "def worker_execute_task(task: Task) -> str:\n",
    "    \"\"\"Worker LLM executes a specific task.\"\"\"\n",
    "    print(f\"  Worker [{task.assignee}] executing task {task.task_id}: {task.description}\")\n",
    "    \n",
    "    # Define prompts based on assignee type\n",
    "    if task.assignee == \"researcher\":\n",
    "        prompt = f\"Research and provide key information or facts related to the following request: {task.description}\"\n",
    "        model = \"gpt-4.1-nano\" # Can use a cheaper model for simple info retrieval\n",
    "        temp = 0.3\n",
    "    elif task.assignee == \"writer\":\n",
    "        prompt = f\"Write a paragraph or section based on the following instruction/topic: {task.description}\"\n",
    "        model = \"gpt-4.1-nano\" # Use a capable model for writing\n",
    "        temp = 0.7\n",
    "    elif task.assignee == \"summarizer\":\n",
    "         prompt = f\"Summarize the following content based on this instruction: {task.description}\"\n",
    "         model = \"gpt-4.1-nano\"\n",
    "         temp = 0.2\n",
    "    else:\n",
    "        return f\"Error: Unknown assignee type: {task.assignee}\"\n",
    "        \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temp\n",
    "        )\n",
    "        result = response.choices[0].message.content.strip()\n",
    "        print(f\"  Worker [{task.assignee}] task {task.task_id} finished.\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"  Worker [{task.assignee}] task {task.task_id} failed: {e}\")\n",
    "        return f\"Task failed: {e}\"\n",
    "\n",
    "def planning_workflow(goal: str) -> List[Task]:\n",
    "    \"\"\"Complete planning and execution workflow\"\"\"\n",
    "    print(\"=== PLANNING PATTERN EXAMPLE ===\")\n",
    "    print(f\"Overall Goal: {goal}\\n\")\n",
    "    \n",
    "    # Step 1: Orchestrator creates the plan\n",
    "    plan = orchestrator_plan(goal)\n",
    "    \n",
    "    if not plan.tasks or plan.tasks[0].status == \"failed\":\n",
    "        print(\"Planning failed. Aborting.\")\n",
    "        return plan.tasks\n",
    "\n",
    "    print(\"\\nExecuting Plan...\")\n",
    "    \n",
    "    # Step 2: Execute tasks sequentially by workers (could be parallelized if tasks are independent)\n",
    "    executed_tasks = []\n",
    "    for task in plan.tasks:\n",
    "        task.status = \"in_progress\"\n",
    "        task.result = worker_execute_task(task)\n",
    "        task.status = \"completed\" if not task.result.startswith(\"Task failed:\") else \"failed\"\n",
    "        executed_tasks.append(task)\n",
    "        print(f\"  Task {task.task_id} Status: {task.status}\\n\")\n",
    "        \n",
    "        # In a real system, results might be passed to subsequent tasks or aggregated\n",
    "        # For this example, we just print the result of each task\n",
    "        print(f\"  Task {task.task_id} Result Snippet: {task.result[:150]}...\\n\")\n",
    "    \n",
    "    print(\"Plan Execution Finished.\")\n",
    "    \n",
    "    # Step 3: (Optional) Orchestrator could aggregate or refine final output\n",
    "    # For simplicity, we just return the executed tasks\n",
    "    \n",
    "    return executed_tasks\n",
    "\n",
    "# Test the planning workflow\n",
    "planning_result = planning_workflow(\"Write a brief report about the benefits of renewable energy, including some key statistics and a concluding summary.\")\n",
    "\n",
    "print(\"\\n=== FINAL EXECUTED TASKS ===\")\n",
    "for task in planning_result:\n",
    "    print(f\"Task {task.task_id} [{task.assignee}] ({task.status}): {task.description}\")\n",
    "    # print(f\"  Result: {task.result[:100]}...\") # Print result snippet\n",
    "    print(\"-\"*20)\n",
    "\n",
    "# Example of aggregating results (Manual for this simple case)\n",
    "final_report_sections = [task.result for task in planning_result if task.status == \"completed\"]\n",
    "print(\"\\n=== AGGREGATED DRAFT REPORT ===\")\n",
    "print(\"\\n\".join(final_report_sections))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2596e0",
   "metadata": {},
   "source": [
    "### 4.4. Multi-Agent\n",
    "Multiple distinct agents, each with a specific role or expertise, collaborate to achieve a common goal. This can involve a coordinator agent or handoff logic.\n",
    "Use Case: Simulate a simple handoff between a Hotel Agent and a Restaurant Agent based on the user's request.\n",
    "We can simulate this by defining system prompts for each agent and using routing/handoff logic (similar to the Routing pattern, but with distinct agent personas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e63ba6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MULTI-AGENT PATTERN EXAMPLE ===\n",
      "Debate Topic: whether AI will improve society\n",
      "Number of turns per agent: 2\n",
      "\n",
      "--- Starting Debate ---\n",
      "  Debater A is generating response...\n",
      "  Debater A generated response.\n",
      "Debater A: Thank you. I firmly believe that AI has the potential to significantly improve society across numerous dimensions. Firstly, AI can enhance healthcare by enabling early diagnosis, personalized treatment plans, and even predicting disease outbreaks, thereby saving lives and reducing costs. Secondly, AI-driven automation can increase productivity and efficiency in industries, freeing humans from monotonous tasks and allowing them to focus on more creative and strategic endeavors. Additionally, AI can facilitate better decision-making through data analysis, helping governments and organizations implement policies that are more effective and equitable. While there are challenges to address, the overall trajectory of AI development promises a future where society benefits from increased innovation, improved quality of life, and greater opportunities for all.\n",
      "\n",
      "  Debater B received message from Debater A.\n",
      "  Debater B is generating response...\n",
      "  Debater B generated response.\n",
      "Debater B: While Debater A presents an optimistic view of AI's potential benefits, I believe there are significant concerns that cannot be overlooked. Firstly, the reliance on AI in healthcare and other critical sectors raises issues of accuracy and accountability. AI systems can make errors, and when these errors occur in medical diagnoses or treatments, the consequences can be dire. Moreover, AI-driven automation threatens widespread job displacement, particularly for low- and middle-skilled workers, potentially increasing economic inequality and social unrest. \n",
      "\n",
      "Additionally, the use of AI in decision-making processes by governments and organizations can lead to biases embedded within algorithms, which may perpetuate discrimination rather than eliminate it. There's also the risk of AI being exploited for malicious purposes, such as cyberattacks, misinformation, or surveillance, infringing on privacy rights and civil liberties. \n",
      "\n",
      "In sum, while AI has potential, its development and deployment must be carefully managed to avoid exacerbating existing societal issues and creating new ones. The benefits are not guaranteed and could come at a considerable cost if not properly regulated.\n",
      "\n",
      "  Debater A received message from Debater B.\n",
      "  Debater A is generating response...\n",
      "  Debater A generated response.\n",
      "Debater A: You raise valid concerns about the risks associated with AI, and they are critical to consider. However, I believe that these challenges do not outweigh the immense potential benefits if we approach AI development responsibly. For example, issues of accuracy and accountability can be mitigated through rigorous testing, transparent algorithms, and robust oversight frameworks. Developing standards and regulations, much like those in medicine and aviation, can help ensure AI systems are safe and reliable.\n",
      "\n",
      "Regarding job displacement, history shows that technological advancements often transform the workforce rather than simply diminish it. With proactive policies—such as retraining programs, education, and social safety nets—we can facilitate a transition that minimizes disruption and creates new opportunities. AI can also augment human capabilities, making work more meaningful and less repetitive.\n",
      "\n",
      "As for biases and malicious use, ongoing research is aimed at creating fairer algorithms and implementing strict ethical guidelines. International cooperation and strong regulatory measures can help prevent misuse and safeguard civil liberties. Ultimately, the societal benefits of AI—like improved healthcare, environmental management, and education—are substantial. With careful development and regulation, we can harness AI to build a more equitable, innovative, and resilient society.\n",
      "\n",
      "  Debater B received message from Debater A.\n",
      "  Debater B is generating response...\n",
      "  Debater B generated response.\n",
      "Debater B: While Debater A emphasizes the importance of regulation and responsible development, the reality is that implementing such measures effectively is incredibly challenging. History shows that technological innovation often outpaces regulation, leading to lagging enforcement and gaps that malicious actors can exploit. For instance, even with strict guidelines, biases embedded in training data can perpetuate discrimination, and transparency in AI algorithms remains difficult to achieve at scale.\n",
      "\n",
      "Furthermore, retraining programs and social safety nets are vital but not sufficient solutions to the rapid pace of job displacement caused by automation. Many workers may lack the resources, time, or skills necessary to transition into new roles, which could exacerbate economic inequalities and social divisions. The assumption that technological progress will automatically lead to societal benefits overlooks the complexities of economic and social dynamics.\n",
      "\n",
      "Additionally, the potential for AI to be used maliciously—such as in mass surveillance or deepfake misinformation—poses serious threats to privacy and democracy. These risks are not merely hypothetical; they are actively being exploited, and the global community lacks comprehensive mechanisms to prevent or respond to such threats effectively.\n",
      "\n",
      "In essence, while AI offers potential benefits, the current and foreseeable risks may outweigh these advantages unless we achieve unprecedented levels of international cooperation, regulation, and societal adaptation—an optimistic scenario that remains uncertain.\n",
      "\n",
      "--- Debate Concluded ---\n",
      "\n",
      "=== CENTRAL DEBATE LOG ===\n",
      "[Debater A -> Debater B]: Thank you. I firmly believe that AI has the potential to significantly improve society across numerous dimensions. Firstly, AI can enhance healthcare by enabling early diagnosis, personalized treatment plans, and even predicting disease outbreaks, thereby saving lives and reducing costs. Secondly, AI-driven automation can increase productivity and efficiency in industries, freeing humans from monotonous tasks and allowing them to focus on more creative and strategic endeavors. Additionally, AI can facilitate better decision-making through data analysis, helping governments and organizations implement policies that are more effective and equitable. While there are challenges to address, the overall trajectory of AI development promises a future where society benefits from increased innovation, improved quality of life, and greater opportunities for all.\n",
      "\n",
      "[Debater B -> Debater A]: While Debater A presents an optimistic view of AI's potential benefits, I believe there are significant concerns that cannot be overlooked. Firstly, the reliance on AI in healthcare and other critical sectors raises issues of accuracy and accountability. AI systems can make errors, and when these errors occur in medical diagnoses or treatments, the consequences can be dire. Moreover, AI-driven automation threatens widespread job displacement, particularly for low- and middle-skilled workers, potentially increasing economic inequality and social unrest. \n",
      "\n",
      "Additionally, the use of AI in decision-making processes by governments and organizations can lead to biases embedded within algorithms, which may perpetuate discrimination rather than eliminate it. There's also the risk of AI being exploited for malicious purposes, such as cyberattacks, misinformation, or surveillance, infringing on privacy rights and civil liberties. \n",
      "\n",
      "In sum, while AI has potential, its development and deployment must be carefully managed to avoid exacerbating existing societal issues and creating new ones. The benefits are not guaranteed and could come at a considerable cost if not properly regulated.\n",
      "\n",
      "[Debater A -> Debater B]: You raise valid concerns about the risks associated with AI, and they are critical to consider. However, I believe that these challenges do not outweigh the immense potential benefits if we approach AI development responsibly. For example, issues of accuracy and accountability can be mitigated through rigorous testing, transparent algorithms, and robust oversight frameworks. Developing standards and regulations, much like those in medicine and aviation, can help ensure AI systems are safe and reliable.\n",
      "\n",
      "Regarding job displacement, history shows that technological advancements often transform the workforce rather than simply diminish it. With proactive policies—such as retraining programs, education, and social safety nets—we can facilitate a transition that minimizes disruption and creates new opportunities. AI can also augment human capabilities, making work more meaningful and less repetitive.\n",
      "\n",
      "As for biases and malicious use, ongoing research is aimed at creating fairer algorithms and implementing strict ethical guidelines. International cooperation and strong regulatory measures can help prevent misuse and safeguard civil liberties. Ultimately, the societal benefits of AI—like improved healthcare, environmental management, and education—are substantial. With careful development and regulation, we can harness AI to build a more equitable, innovative, and resilient society.\n",
      "\n",
      "[Debater B -> Debater A]: While Debater A emphasizes the importance of regulation and responsible development, the reality is that implementing such measures effectively is incredibly challenging. History shows that technological innovation often outpaces regulation, leading to lagging enforcement and gaps that malicious actors can exploit. For instance, even with strict guidelines, biases embedded in training data can perpetuate discrimination, and transparency in AI algorithms remains difficult to achieve at scale.\n",
      "\n",
      "Furthermore, retraining programs and social safety nets are vital but not sufficient solutions to the rapid pace of job displacement caused by automation. Many workers may lack the resources, time, or skills necessary to transition into new roles, which could exacerbate economic inequalities and social divisions. The assumption that technological progress will automatically lead to societal benefits overlooks the complexities of economic and social dynamics.\n",
      "\n",
      "Additionally, the potential for AI to be used maliciously—such as in mass surveillance or deepfake misinformation—poses serious threats to privacy and democracy. These risks are not merely hypothetical; they are actively being exploited, and the global community lacks comprehensive mechanisms to prevent or respond to such threats effectively.\n",
      "\n",
      "In essence, while AI offers potential benefits, the current and foreseeable risks may outweigh these advantages unless we achieve unprecedented levels of international cooperation, regulation, and societal adaptation—an optimistic scenario that remains uncertain.\n",
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Dict\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, name: str, role: str, description: str, model: str = \"gpt-4o-mini\"):\n",
    "        self.name = name\n",
    "        self.role = role\n",
    "        self.description = description\n",
    "        self.model = model\n",
    "        self.conversation_history = [{\n",
    "            \"role\": \"system\", \n",
    "            \"content\": f\"You are {self.name}, a {self.role}. Your goal is to {self.description}. Stay in character and contribute to the discussion.\"\n",
    "        }]\n",
    "    \n",
    "    def send_message(self, recipient_name: str, content: str):\n",
    "        \"\"\"Simulate sending a message to another agent.\"\"\"\n",
    "        message = {\"sender\": self.name, \"recipient\": recipient_name, \"content\": content}\n",
    "        # In a real system, this would go into a shared message queue or similar\n",
    "        # For this simulation, we just return the message\n",
    "        return message\n",
    "        \n",
    "    def receive_message(self, message: Dict[str, str]):\n",
    "        \"\"\"Integrate a received message into conversation history.\"\"\"\n",
    "        # Add message from other agent to history as 'user' role from recipient's perspective\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": f\"Message from {message['sender']}: {message['content']}\"})\n",
    "        print(f\"  {self.name} received message from {message['sender']}.\")\n",
    "        \n",
    "    def generate_response(self, query: str = None) -> str:\n",
    "        \"\"\"Generate a response based on current history and optional query.\"\"\"\n",
    "        messages_for_api = list(self.conversation_history) # Copy history\n",
    "        if query:\n",
    "            messages_for_api.append({\"role\": \"user\", \"content\": query})\n",
    "            \n",
    "        print(f\"  {self.name} is generating response...\")\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages_for_api,\n",
    "                temperature=0.7 # Allow creativity in interaction\n",
    "            )\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Add the generated response to the agent's history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": content})\n",
    "            print(f\"  {self.name} generated response.\")\n",
    "            return content\n",
    "        except Exception as e:\n",
    "            print(f\"  {self.name} failed to generate response: {e}\")\n",
    "            return f\"[Agent {self.name} Error: {e}]\"\n",
    "\n",
    "def multi_agent_debate(topic: str, num_turns: int = 3):\n",
    "    \"\"\"Simulate a multi-agent debate.\"\"\"\n",
    "    print(\"=== MULTI-AGENT PATTERN EXAMPLE ===\")\n",
    "    print(f\"Debate Topic: {topic}\")\n",
    "    print(f\"Number of turns per agent: {num_turns}\\n\")\n",
    "    \n",
    "    # Initialize agents\n",
    "    agent_a = Agent(\n",
    "        name=\"Debater A\",\n",
    "        role=\"proponent\",\n",
    "        description=f\"argue in favor of the topic '{topic}'. Respond to Debater B's points.\",\n",
    "        model=\"gpt-4.1-nano\"\n",
    "    )\n",
    "    \n",
    "    agent_b = Agent(\n",
    "         name=\"Debater B\",\n",
    "        role=\"opponent\",\n",
    "        description=f\"argue against the topic '{topic}'. Respond to Debater A's points.\",\n",
    "        model=\"gpt-4.1-nano\"\n",
    "    )\n",
    "    \n",
    "    agents = [agent_a, agent_b]\n",
    "    \n",
    "    # Start the debate\n",
    "    central_log = [] # A central place to log messages\n",
    "    \n",
    "    # Initial statement from Agent A\n",
    "    print(\"--- Starting Debate ---\")\n",
    "    initial_query = f\"Begin the debate. State your opening argument for the topic: {topic}\"\n",
    "    response_a = agent_a.generate_response(initial_query)\n",
    "    central_log.append(agent_a.send_message(agent_b.name, response_a))\n",
    "    print(f\"{agent_a.name}: {response_a}\\n\")\n",
    "    \n",
    "    # Debate turns\n",
    "    for i in range(num_turns * len(agents) - 1): # Total turns - 1 (first turn already done)\n",
    "        current_agent_idx = (i + 1) % len(agents)\n",
    "        current_agent = agents[current_agent_idx]\n",
    "        previous_agent = agents[i % len(agents)]\n",
    "        \n",
    "        # In a real system, agents would pick up messages from a queue\n",
    "        # Here, we manually pass the last message\n",
    "        last_message = central_log[-1]\n",
    "        if last_message['recipient'] == current_agent.name:\n",
    "             current_agent.receive_message(last_message)\n",
    "        \n",
    "        response = current_agent.generate_response()\n",
    "        \n",
    "        # Determine recipient (the other agent)\n",
    "        recipient_agent = agents[(current_agent_idx + 1) % len(agents)]\n",
    "        central_log.append(current_agent.send_message(recipient_agent.name, response))\n",
    "        print(f\"{current_agent.name}: {response}\\n\")\n",
    "        \n",
    "        # Small delay to simulate thinking/processing time (optional)\n",
    "        # time.sleep(1)\n",
    "\n",
    "    print(\"--- Debate Concluded ---\")\n",
    "\n",
    "    # (Optional) A final agent could summarize the debate\n",
    "    # For this example, we just print the log\n",
    "    print(\"\\n=== CENTRAL DEBATE LOG ===\")\n",
    "    for msg in central_log:\n",
    "        print(f\"[{msg['sender']} -> {msg['recipient']}]: {msg['content']}\\n\")\n",
    "        \n",
    "    return central_log\n",
    "\n",
    "# Test the multi-agent pattern\n",
    "debate_log = multi_agent_debate(\"whether AI will improve society\", num_turns=2)\n",
    "\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6367c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided practical examples of several common agentic design patterns using the OpenAI API. By combining these patterns, more sophisticated and autonomous AI systems can be built.\n",
    "\n",
    "- **Prompt Chaining:** Sequential processing.\n",
    "- **Routing/Handoff:** Directing tasks to specialized models/agents.\n",
    "- **Parallelization:** Concurrent execution for efficiency.\n",
    "- **Reflection:** Iterative self-improvement.\n",
    "- **Tool Use:** Interacting with external environments.\n",
    "- **Planning:** Decomposing complex goals into executable steps.\n",
    "- **Multi-Agent:** Collaboration and interaction between independent agents.\n",
    "\n",
    "Exploring combinations and variations of these patterns is key to building powerful agentic applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
